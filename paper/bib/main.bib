@article{antoniou2010,
  ids = {antoniou},
  title = {Web 2.0 Geotagged Photos: {{Assessing}} the Spatial Dimension of the Phenomenon},
  shorttitle = {Web 2.0 Geotagged Photos},
  author = {Antoniou, Vyron and Morley, Jeremy and Haklay, Muki},
  year = {2010},
  month = jan,
  journal = {Geomatica},
  volume = {64},
  pages = {99--110},
  abstract = {Among popular Web 2.0 applications are the social networking, photo-sharing websites like Flickr, Panoramio, Picasa Web, and Geograph. The phenomenon of user-generated content and the increased presence of geographic information in such applications have motivated researchers to consider them as a source of geographic information. In this paper we question whether such web applications can serve as reliable sources of spatial content. We differentiate between spatially explicit and implicit applications, in accordance to their declared aims, and evaluate if they have an impact on the spatial distribution of geotagged photos for Great Britain. We also compare the spatial distribution of the photos submitted with population data, and the patterns of contribution to these sources over a period of 18 months. Finally, at a larger scale, we examine the spatial distribution of photos and their spatial density for 15 test areas and look into issues such as data currency and user behaviour. Our findings show that only web applications that urge users to interact directly with spatial entities can serve as reliable, universal sources of spatial content.},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/antoniou_et_al_2010.pdf}
}

@incollection{auer2007,
  title = {{{DBpedia}}: {{A Nucleus}} for a {{Web}} of {{Open Data}}},
  shorttitle = {{{DBpedia}}},
  booktitle = {The {{Semantic Web}}},
  author = {Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and {Cudr{\'e}-Mauroux}, Philippe},
  year = {2007},
  volume = {4825},
  pages = {722--735},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-76298-0_52},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  isbn = {978-3-540-76297-3 978-3-540-76298-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/auer_et_al_2007.pdf}
}

@article{barbieri2020,
  title = {{{TweetEval}}: {{Unified Benchmark}} and {{Comparative Evaluation}} for {{Tweet Classification}}},
  shorttitle = {{{TweetEval}}},
  author = {Barbieri, Francesco and {Camacho-Collados}, Jose and Neves, Leonardo and {Espinosa-Anke}, Luis},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.12421 [cs]},
  eprint = {2010.12421},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domainspecific data. In this paper, we propose a new evaluation framework (TWEETEVAL) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pretrained generic language models, and continue training them on Twitter corpora.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {/home/cjber/drive/pdf/barbieri_et_al_2020.pdf}
}

@article{buscaldi2011,
  title = {Approaches to Disambiguating Toponyms},
  author = {Buscaldi, Davide},
  year = {2011},
  month = jul,
  journal = {SIGSPATIAL Special},
  volume = {3},
  number = {2},
  pages = {16--19},
  issn = {19467729},
  doi = {10.1145/2047296.2047300},
  langid = {english},
  keywords = {TD},
  file = {/home/cjber/drive/pdf/buscaldi_2011.pdf;/home/cjber/drive/pdf/buscaldi_22.pdf}
}

@article{couclelis2010,
  title = {Ontologies of Geographic Information},
  author = {Couclelis, Helen},
  year = {2010},
  month = nov,
  journal = {International Journal of Geographical Information Science},
  volume = {24},
  number = {12},
  pages = {1785--1809},
  issn = {1365-8816, 1362-3087},
  doi = {10.1080/13658816.2010.484392},
  langid = {english},
  file = {/home/cjber/drive/pdf/couclelis_2010.pdf}
}

@article{delozier2015,
  title = {Gazetteer-{{Independent Toponym Resolution Using Geographic Word Profiles}}},
  author = {DeLozier, Grant and Baldridge, Jason and London, Loretta},
  year = {2015},
  pages = {7},
  abstract = {Toponym resolution, or grounding names of places to their actual locations, is an important problem in analysis of both historical corpora and present-day news and web content. Recent approaches have shifted from rule-based spatial minimization methods to machine learned classifiers that use features of the text surrounding a toponym. Such methods have been shown to be highly effective, but they crucially rely on gazetteers and are unable to handle unknown place names or locations. We address this limitation by modeling the geographic distributions of words over the earth's surface: we calculate the geographic profile of each word based on local spatial statistics over a set of geo-referenced language models. These geo-profiles can be further refined by combining in-domain data with background statistics from Wikipedia. Our resolver computes the overlap of all geo-profiles in a given text span; without using a gazetteer, it performs on par with existing classifiers. When combined with a gazetteer, it achieves state-of-the-art performance for two standard toponym resolution corpora (TR-CoNLL and Civil War). Furthermore, it dramatically improves recall when toponyms are identified by named entity recognizers, which often (correctly) find non-standard variants of toponyms.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/delozier_et_al_2015.pdf}
}

@article{devlin2019,
  ids = {devlin2019a},
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/devlin_et_al_2019.pdf;/home/cjber/drive/pdf/devlin_et_al_22.pdf;/home/cjber/.zotero/storage/XFVJWZDY/1810.html}
}

@article{dror2018a,
  title = {Appendix - {{Recommended Statistical Significance Tests}} for {{NLP Tasks}}},
  author = {Dror, Rotem and Reichart, Roi},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.01448 [cs]},
  eprint = {1809.01448},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Statistical significance testing plays an important role when drawing conclusions from experimental results in NLP papers. Particularly, it is a valuable tool when one would like to establish the superiority of one algorithm over another. This appendix complements the guide for testing statistical significance in NLP presented in [5] by proposing valid statistical tests for the common tasks and evaluation measures in the field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/dror_reichart_2018.pdf}
}

@article{gao2013,
  title = {Towards {{Platial Joins}} and {{Buffers}} in {{Place-Based GIS}}},
  author = {Gao, Song and Janowicz, Krzysztof and McKenzie, Grant and Li, Linna},
  year = {2013},
  pages = {8},
  abstract = {Place-based GIS are still a novel research topic and break with some traditions of established systems. The typical spatial perspective is based on geometric reference systems that include coordinates, distances, topology, and directions; while the alternative platial perspective is usually characterized by place names and descriptions as well as semantic relationships between places. In past decades, spacebased geographic information systems have made significant progress in terms of theories, models, functionalities, and applications. In contrast, place-based GIS are not yet well developed, although there is an increasing interest in platial and especially relational approaches. In this paper we take an example-driven, first step towards introducing placebased versions of the well known spatial join and buffer operations, and apply them to deal with place-based semantic compression and expansion in DBpedia.},
  langid = {english},
  keywords = {⛔ No DOI found,Platial},
  file = {/home/cjber/drive/pdf/gao_et_al_2013.pdf}
}

@article{gao2017,
  title = {A Data-Synthesis-Driven Method for Detecting and Extracting Vague Cognitive Regions},
  author = {Gao, Song and Janowicz, Krzysztof and Montello, Daniel R. and Hu, Yingjie and Yang, Jiue-An and McKenzie, Grant and Ju, Yiting and Gong, Li and Adams, Benjamin and Yan, Bo},
  year = {2017},
  month = jun,
  journal = {International Journal of Geographical Information Science},
  volume = {31},
  number = {6},
  pages = {1245--1271},
  issn = {13658816},
  doi = {10.1080/13658816.2016.1273357},
  abstract = {Cognitive regions and places are notoriously difficult to represent in geographic information science and systems. The exact delineation of cognitive regions is challenging insofar as borders are vague, membership within the regions varies non-monotonically, and raters cannot be assumed to assess membership consistently and homogeneously. In a study published in this journal in 2014, researchers devised a novel grid-based task in which participants rated the membership of individual cells in a given region and contrasted this approach to a standard boundary-drawing task. Specifically, the authors assessed the vague cognitive regions ofNorthern CaliforniaandSouthern California. The boundary between these cognitive regions was found to have variable width, and region membership peaked not at the most northern or southern cells but at substantially less extreme latitudes. The authors thus concluded that region membership is about attitude, not just latitude. In the present work, we reproduce this study by approaching it from a computationalfourth-paradigmperspective, i.e., by the synthesis of high volumes of heterogeneous data from various sources. We compare the regions which we identify to those from the human-participants study of 2014, identifying differences and commonalities. Our results show a significant positive correlation to those in the original study. Beyond the extracted regions themselves, we compare and contrast the empirical and analytical approaches of these two methods, one a conventional human-participants study and the other an application of increasingly popular data-synthesis-driven research methods in GIScience.},
  keywords = {cognitive regions,Computational acoustics,Correlation (Statistics),Data analysis,data synthesis,Information science,latent Dirichlet allocation,Multivariate analysis,Place,vagueness},
  file = {/home/cjber/drive/pdf/Gao et al_2017_A data-synthesis-driven method for detecting and extracting vague cognitive.pdf;/home/cjber/drive/pdf/Gao et al_2017_A data-synthesis-driven method for detecting and extracting vague cognitive2.pdf}
}

@inproceedings{gardner2018,
  title = {{{AllenNLP}}: {{A Deep Semantic Natural Language Processing Platform}}},
  shorttitle = {{{AllenNLP}}},
  booktitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP-OSS}})},
  author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson F. and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke},
  year = {2018},
  pages = {1--6},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/w18-2501},
  abstract = {This paper describes AllenNLP, a platform for research on deep learning methods in natural language understanding. AllenNLP is designed to support researchers who want to build novel language understanding models quickly and easily. It is built on top of PyTorch, allowing for dynamic computation graphs, and provides (1) a flexible data API that handles intelligent batching and padding, (2) highlevel abstractions for common operations in working with text, and (3) a modular and extensible experiment framework that makes doing good science easy. It also includes reference implementations of high quality approaches for both core semantic problems (e.g. semantic role labeling (Palmer et al., 2005)) and language understanding applications (e.g. machine comprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source effort maintained by engineers and researchers at the Allen Institute for Artificial Intelligence.},
  langid = {english},
  file = {/home/cjber/drive/pdf/gardner_et_al_2018.pdf}
}

@article{goodchild2007,
  ids = {goodchild2007a},
  title = {Citizens as Sensors: The World of Volunteered Geography},
  shorttitle = {Citizens as Sensors},
  author = {Goodchild, Michael F.},
  year = {2007},
  journal = {GeoJournal},
  volume = {69},
  number = {4},
  pages = {211--221},
  publisher = {{Springer}},
  issn = {0343-2521},
  doi = {10.1007/s10708-007-9111-y},
  abstract = {In recent months there has been an explosion of interest in using the Web to create, assemble, and disseminate geographic information provided voluntarily by individuals. Sites such as Wikimapia and OpenStreetMap are empowering citizens to create a global patchwork of geographic information, while Google Earth and other virtual globes are encouraging volunteers to develop interesting applications using their own data. I review this phenomenon, and examine associated issues: what drives people to do this, how accurate are the results, will they threaten individual privacy, and how can they augment more conventional sources? I compare this new phenomenon to more traditional citizen science and the role of the amateur in geographic observation.},
  annotation = {2502 citations (Crossref) [2022-07-27]},
  file = {/home/cjber/drive/pdf/goodchild_2007.pdf}
}

@incollection{goodchild2011,
  ids = {goodchild2011a},
  title = {Formalizing {{Place}} in {{Geographic Information Systems}}},
  booktitle = {Communities, {{Neighborhoods}}, and {{Health}}},
  author = {Goodchild, Michael F.},
  editor = {Burton, Linda M. and Matthews, Stephen A. and Leung, ManChui and Kemp, Susan P. and Takeuchi, David T.},
  year = {2011},
  pages = {21--33},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-7482-2_2},
  isbn = {978-1-4419-7481-5 978-1-4419-7482-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/goodchild_2011.pdf;/home/cjber/drive/pdf/goodchild_22.pdf}
}

@inproceedings{gritta2017a,
  title = {Vancouver {{Welcomes You}}! {{Minimalist Location Metonymy Resolution}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gritta, Milan and Pilehvar, Mohammad Taher and Limsopatham, Nut and Collier, Nigel},
  year = {2017},
  pages = {1248--1259},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/p17-1115},
  abstract = {Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.},
  langid = {english},
  file = {/home/cjber/drive/pdf/gritta_et_al_2017.pdf}
}

@article{gritta2020,
  title = {A Pragmatic Guide to Geoparsing Evaluation: {{Toponyms}}, {{Named Entity Recognition}} and Pragmatics},
  shorttitle = {A Pragmatic Guide to Geoparsing Evaluation},
  author = {Gritta, Milan and Pilehvar, Mohammad Taher and Collier, Nigel},
  year = {2020},
  month = sep,
  journal = {Language Resources and Evaluation},
  volume = {54},
  number = {3},
  pages = {683--712},
  issn = {1574-020X, 1574-0218},
  doi = {10.1007/s10579-019-09475-3},
  abstract = {Empirical methods in geoparsing have thus far lacked a standard evaluation framework describing the task, metrics and data used to compare state-of-theart systems. Evaluation is further made inconsistent, even unrepresentative of real world usage by the lack of distinction between the different types of toponyms, which necessitates new guidelines, a consolidation of metrics and a detailed toponym taxonomy with implications for Named Entity Recognition (NER) and beyond. To address these deficiencies, our manuscript introduces a new framework in three parts. (Part 1) Task Definition: clarified via corpus linguistic analysis proposing a fine-grained Pragmatic Taxonomy of Toponyms. (Part 2) Metrics: discussed and reviewed for a rigorous evaluation including recommendations for NER/Geoparsing practitioners. (Part 3) Evaluation data: shared via a new dataset called GeoWebNews to provide test/train examples and enable immediate use of our contributions. In addition to fine-grained Geotagging and Toponym Resolution (Geocoding), this dataset is also suitable for prototyping and evaluating machine learning NLP models.},
  langid = {english},
  keywords = {TD},
  file = {/home/cjber/.zotero/storage/P8UAQMAY/Gritta et al_2019_A pragmatic guide to geoparsing evaluation2.pdf;/home/cjber/drive/pdf/gritta_et_al_2020.pdf;/home/cjber/drive/pdf/gritta_et_al_3.pdf}
}

@article{halterman2017,
  title = {Mordecai: {{Full}} Text Geoparsing and Event Geocoding},
  author = {Halterman, Andrew},
  year = {2017},
  journal = {The Journal of Open Source Software},
  volume = {2},
  number = {9},
  doi = {10.21105/joss.00091}
}

@article{hollenstein2010,
  title = {Exploring Place through User-Generated Content: {{Using Flickr}} Tags to Describe City Cores},
  shorttitle = {Exploring Place through User-Generated Content},
  author = {Hollenstein, Livia and Purves, Ross},
  year = {2010},
  month = dec,
  journal = {Journal of Spatial Information Science},
  number = {1},
  pages = {21--48},
  issn = {1948-660X},
  abstract = {Terms used to describe city centers, such as Downtown, are key concepts in everyday or vernacular language. Here, we explore such language by harvesting georeferenced and tagged metadata associated with 8 million Flickr images and thus consider how large numbers of people name city core areas. The nature of errors and imprecision in tagging and georeferencing are quantified, and automatically generated precision measures appear to mirror errors in the positioning of images. Users seek to ascribe appropriate semantics to images, though bulk-uploading and bulk-tagging may introduce bias. Between 0.5-2\% of tags associated with georeferenced images analyzed describe city core areas generically, while 70\% of all georeferenced images analyzed include specific place name tags, with place names at the granularity of city names being by far the most common. Using Flickr metadata, it is possible not only to describe the use of the term Downtown across the USA, but also to explore the borders of city center neighborhoods at the level of individual cities, whilst accounting for bias by the use of tag profiles.},
  copyright = {Copyright (c) 2010 Journal of Spatial Information Science},
  langid = {english},
  keywords = {⛔ No DOI found,Flickr},
  file = {/home/cjber/drive/pdf/hollenstein_purves_2010.pdf}
}

@inproceedings{honnibal2015,
  title = {An {{Improved Non-monotonic Transition System}} for {{Dependency Parsing}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Honnibal, Matthew and Johnson, Mark},
  year = {2015},
  pages = {1373--1378},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10.18653/v1/d15-1162},
  abstract = {Transition-based dependency parsers usually use transition systems that monotonically extend partial parse states until they identify a complete parse tree. Honnibal et al. (2013) showed that greedy onebest parsing accuracy can be improved by adding additional non-monotonic transitions that permit the parser to ``repair'' earlier parsing mistakes by ``over-writing'' earlier parsing decisions. This increases the size of the set of complete parse trees that each partial parse state can derive, enabling such a parser to escape the ``garden paths'' that can trap monotonic greedy transition-based dependency parsers.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Honnibal_Johnson_2015_An Improved Non-monotonic Transition System for Dependency Parsing.pdf}
}

@article{honnibal2017,
  title = {Spacy 2: {{Natural}} Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  journal = {To appear},
  volume = {7},
  number = {1},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{hu2019,
  title = {A Natural Language Processing and Geospatial Clustering Framework for Harvesting Local Place Names from Geotagged Housing Advertisements},
  author = {Hu, Yingjie and Mao, Huina and McKenzie, Grant},
  year = {2019},
  month = apr,
  journal = {International Journal of Geographical Information Science},
  volume = {33},
  number = {4},
  pages = {714--738},
  issn = {1365-8816, 1362-3087},
  doi = {10.1080/13658816.2018.1458986},
  abstract = {Local place names are frequently used by residents living in a geographic region. Such place names may not be recorded in existing gazetteers, due to their vernacular nature, relative insignificance to a gazetteer covering a large area (e.g. the entire world), recent establishment (e.g. the name of a newly-opened shopping center) or other reasons. While not always recorded, local place names play important roles in many applications, from supporting public participation in urban planning to locating victims in disaster response. In this paper, we propose a computational framework for harvesting local place names from geotagged housing advertisements. We make use of those advertisements posted on local-oriented websites, such as Craigslist, where local place names are often mentioned. The proposed framework consists of two stages: natural language processing (NLP) and geospatial clustering. The NLP stage examines the textual content of housing advertisements and extracts place name candidates. The geospatial stage focuses on the coordinates associated with the extracted place name candidates and performs multiscale geospatial clustering to filter out the non-place names. We evaluate our framework by comparing its performance with those of six baselines. We also compare our result with four existing gazetteers to demonstrate the not-yet-recorded local place names discovered by our framework.},
  langid = {english},
  annotation = {25 citations (Crossref) [2022-07-27]},
  file = {/home/cjber/drive/pdf/Hu et al_2019_A natural language processing and geospatial clustering framework for2.pdf;/home/cjber/drive/pdf/hu_et_al_2019.pdf}
}

@article{karimzadeh2019,
  title = {{{GeoTxt}}: {{A}} Scalable Geoparsing System for Unstructured Text Geolocation},
  shorttitle = {{{GeoTxt}}},
  author = {Karimzadeh, Morteza and Pezanowski, Scott and MacEachren, Alan M. and Wallgr{\"u}n, Jan O.},
  year = {2019},
  month = feb,
  journal = {Transactions in GIS},
  volume = {23},
  number = {1},
  pages = {118--136},
  issn = {1361-1682, 1467-9671},
  doi = {10.1111/tgis.12510},
  abstract = {In this article we present GeoTxt, a scalable geoparsing system for the recognition and geolocation of place names in unstructured text. GeoTxt offers six named entity recognition (NER) algorithms for place name recognition, and utilizes an enterprise search engine for the indexing, ranking, and retrieval of toponyms, enabling scalable geoparsing for streaming text. GeoTxt offers a flexible application programming interface (API), allowing for customized attribute and/or spatial ranking of retrieved toponyms. We evaluate the system on a corpus of manually geo-annotated tweets. First, we benchmark the performance of the six NERs that GeoTxt provides access to. Second, we assess GeoTxt toponym resolution accuracy incrementally, demonstrating improvements in toponym resolution achieved (or not achieved) by adding specific heuristics and disambiguation methods. Compared to using the GeoNames web service, GeoTxt's toponym resolution demonstrates a 20\% accuracy gain. Our results show that places mentioned in the same tweet do not tend to be geographically proximate.},
  langid = {english},
  file = {/home/cjber/drive/pdf/karimzadeh_et_al_2019.pdf}
}

@article{kingma2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/kingma_ba_2017.pdf}
}

@article{kumar2019,
  title = {Location Reference Identification from Tweets during Emergencies: {{A}} Deep Learning Approach},
  shorttitle = {Location Reference Identification from Tweets during Emergencies},
  author = {Kumar, Abhinav and Singh, Jyoti Prakash},
  year = {2019},
  month = feb,
  journal = {International Journal of Disaster Risk Reduction},
  volume = {33},
  pages = {365--375},
  issn = {22124209},
  doi = {10.1016/j.ijdrr.2018.10.021},
  abstract = {Twitter is recently being used during crises to communicate with officials and provide rescue and relief operation in real time. The geographical location information of the event, as well as users, are vitally important in such scenarios. The identification of geographic location is one of the challenging tasks as the location information fields, such as user location and place name of tweets are not reliable. The extraction of location information from tweet text is difficult as it contains a lot of non-standard English, grammatical errors, spelling mistakes, non-standard abbreviations, and so on. This research aims to extract location words used in the tweet using a Convolutional Neural Network (CNN) based model. We achieved the exact matching score of 0.929, Hamming loss of 0.002, and F1-score of 0.96 for the tweets related to the earthquake. Our model was able to extract even three- to four-word long location references which is also evident from the exact matching score of over 92\%. The findings of this paper can help in early event localization, emergency situations, real-time road traffic management, localized advertisement, and in various location-based services.},
  langid = {english},
  file = {/home/cjber/drive/pdf/kumar_singh_2019.pdf}
}

@article{lample2016,
  ids = {lample2016a},
  title = {Neural {{Architectures}} for {{Named Entity Recognition}}},
  author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
  year = {2016},
  month = apr,
  journal = {arXiv:1603.01360 [cs]},
  eprint = {1603.01360},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/lample_et_al_2016.pdf;/home/cjber/drive/pdf/lample_et_al_22.pdf;/home/cjber/.zotero/storage/9XEQA8SE/1603.html}
}

@article{leidner2008,
  title = {Toponym {{Resolution}} in {{Text}}},
  author = {Leidner, Jochen Lothar},
  year = {2008},
  pages = {287},
  doi = {10.1145/1328964.1328989},
  abstract = {Background. In the area of Geographic Information Systems (GIS), a shared discipline between informatics and geography, the term geo-parsing is used to describe the process of identifying names in text, which in computational linguistics is known as named entity recognition and classification (NERC). The term geo-coding is used for the task of mapping from implicitly geo-referenced datasets (such as structured address records) to explicitly geo-referenced representations (e.g., using latitude and longitude). However, present-day GIS systems provide no automatic geo-coding functionality for unstructured text.},
  langid = {english},
  file = {/home/cjber/drive/pdf/leidner_2008.pdf}
}

@article{liu2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/liu_et_al_2019.pdf;/home/cjber/.zotero/storage/PZWRCFSJ/1907.html}
}

@article{loshchilov2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  journal = {arXiv:1711.05101 [cs, math]},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/home/cjber/drive/pdf/loshchilov_hutter_2019.pdf;/home/cjber/.zotero/storage/QJVURUWK/1711.html}
}

@article{mani2010,
  ids = {mani2010a},
  title = {{{SpatialML}}: Annotation Scheme, Resources, and Evaluation},
  shorttitle = {{{SpatialML}}},
  author = {Mani, Inderjeet and Doran, Christy and Harris, Dave and Hitzeman, Janet and Quimby, Rob and Richer, Justin and Wellner, Ben and Mardis, Scott and Clancy, Seamus},
  year = {2010},
  month = sep,
  journal = {Language Resources and Evaluation},
  volume = {44},
  number = {3},
  pages = {263--280},
  issn = {1574-020X, 1574-0218},
  doi = {10.1007/s10579-010-9121-0},
  langid = {english},
  file = {/home/cjber/.zotero/storage/Q3VUBRJA/Mani et al_2010_SpatialML2.pdf;/home/cjber/drive/pdf/mani_et_al_2010.pdf}
}

@article{mikolov2013,
  title = {Efficient Estimation of Word Representations in Vector Space},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  journal = {arXiv preprint arXiv:1301.3781},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {\#nosource,⛔ No DOI found}
}

@inproceedings{moncla2014,
  title = {Geocoding for Texts with Fine-Grain Toponyms: An Experiment on a Geoparsed Hiking Descriptions Corpus},
  shorttitle = {Geocoding for Texts with Fine-Grain Toponyms},
  booktitle = {Proceedings of the 22nd {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  author = {Moncla, Ludovic and {Renteria-Agualimpia}, Walter and {Nogueras-Iso}, Javier and Gaio, Mauro},
  year = {2014},
  month = nov,
  pages = {183--192},
  publisher = {{ACM}},
  address = {{Dallas Texas}},
  doi = {10.1145/2666310.2666386},
  abstract = {Geoparsing and geocoding are two essential middleware services to facilitate final user applications such as locationaware searching or different types of location-based services. The objective of this work is to propose a method for establishing a processing chain to support the geoparsing and geocoding of text documents describing events strongly linked with space and with a frequent use of fine-grain toponyms. The geoparsing part is a Natural Language Processing approach which combines the use of part of speech and syntactico-semantic combined patterns (cascade of transducers). However, the real novelty of this work lies in the geocoding method. The geocoding algorithm is unsupervised and takes profit of clustering techniques to provide a solution for disambiguating the toponyms found in gazetteers, and at the same time estimating the spatial footprint of those other fine-grain toponyms not found in gazetteers. The feasibility of the proposal has been tested with a corpus of hiking descriptions in French, Spanish and Italian.},
  isbn = {978-1-4503-3131-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/moncla_et_al_2014.pdf;/home/cjber/drive/pdf/moncla_et_al_22.pdf}
}

@article{nakayama2018,
  title = {Doccano: Text Annotation for Humans},
  author = {Nakayama, Hiroki and Kubo, Takahiro and Kamura, Junya and Taniguchi, Yasufumi and Liang, Xu},
  year = {2018},
  keywords = {⛔ No DOI found}
}

@article{nguyen2020,
  title = {{{BERTweet}}: {{A}} Pre-Trained Language Model for {{English Tweets}}},
  shorttitle = {{{BERTweet}}},
  author = {Nguyen, Dat Quoc and Vu, Thanh and Nguyen, Anh Tuan},
  year = {2020},
  month = oct,
  journal = {arXiv:2005.10200 [cs]},
  eprint = {2005.10200},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present BERTweet, the first public largescale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERTbase (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTabase and XLM-Rbase (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at: https://github.com/ VinAIResearch/BERTweet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/nguyen_et_al_2020.pdf}
}

@incollection{paszke2019,
  title = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{pennington2014,
  ids = {pennington2014a},
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/d14-1162},
  file = {/home/cjber/drive/pdf/pennington_et_al_2014.pdf;/home/cjber/drive/pdf/pennington_et_al_22.pdf}
}

@article{peters2018,
  ids = {peters2018a},
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  journal = {arXiv:1802.05365 [cs]},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/peters_et_al_2018.pdf;/home/cjber/drive/pdf/peters_et_al_2018.pdf】;/home/cjber/drive/pdf/peters_et_al_22.pdf;/home/cjber/.zotero/storage/FH7LJAV5/1802.html}
}

@book{purves2018,
  title = {Geographic {{Information Retrieval}}: {{Progress}} and {{Challenges}} in {{Spatial Search}} of {{Text}}},
  author = {Purves, Ross S. and Clough, Paul and Jones, Christopher B. and Hall, Mark H. and Murdock, Vanessa},
  year = {2018},
  publisher = {{now}},
  doi = {10.1561/1500000034},
  abstract = {Significant amounts of information available today contain references to places on earth. Traditionally such information has been held as structured data and was the concern of Geographic Information Systems (GIS). However, increasing amounts of data in the form of unstructured text are available for indexing and retrieval that also contain spatial references. This monograph describes the field of Geographic Information Retrieval (GIR) that seeks to develop spatially-aware search systems and support user's geographical information needs. Important concepts with respect to storing, querying and analysing geographical information in computers are introduced, before user needs and interaction in the context of GIR are explored. The task of associating documents with coordinates, prior to their indexing and ranking forms the core of any GIR system, and different approaches and their implications are discussed. Evaluating the resulting systems and their components, and different paradigms for doing so continue to be an important area of research in GIR and are illustrated through several examples. The monograph provides an overview of the research field, and in so doing identifies key remaining research challenges in GIR.},
  isbn = {978-1-68083-413-0},
  file = {/home/cjber/drive/pdf/purves_et_al_2018.pdf}
}

@inproceedings{pustejovsky2015,
  title = {{{SemEval-2015 Task}} 8: {{SpaceEval}}},
  shorttitle = {{{SemEval-2015 Task}} 8},
  booktitle = {Proceedings of the 9th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval}} 2015)},
  author = {Pustejovsky, James and Kordjamshidi, Parisa and Moens, Marie-Francine and Levine, Aaron and Dworman, Seth and Yocum, Zachary},
  year = {2015},
  pages = {884--894},
  publisher = {{Association for Computational Linguistics}},
  address = {{Denver, Colorado}},
  doi = {10.18653/v1/s15-2149},
  abstract = {Human languages exhibit a variety of strategies for communicating spatial information, including toponyms, spatial nominals, locations that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task.},
  langid = {english},
  file = {/home/cjber/drive/pdf/pustejovsky_et_al_2015.pdf}
}

@incollection{pustejovsky2017,
  title = {{{ISO-Space}}: {{Annotating Static}} and {{Dynamic Spatial Information}}},
  shorttitle = {{{ISO-Space}}},
  booktitle = {Handbook of {{Linguistic Annotation}}},
  author = {Pustejovsky, James},
  editor = {Ide, Nancy and Pustejovsky, James},
  year = {2017},
  pages = {989--1024},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-024-0881-2_37},
  abstract = {An understanding of spatial information in natural language is necessary for many computational linguistics and artificial intelligence applications. In this chapter, we describe an annotation scheme for the markup of spatial relations, both static and dynamic, as expressed in text and other media. The desiderata for such a specification language are presented along with what representational mechanisms are required for such a specification to be successful. We review the annotation development process, and the adoption of the initial specification ISOspace, as an ISO standard, renamed ISOspace. We conclude with a discussion of the use of ISOspace in the context of the shared task SpaceEval 2015.},
  isbn = {978-94-024-0879-9 978-94-024-0881-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/pustejovsky_2017.pdf;/home/cjber/drive/pdf/pustejovsky_22.pdf}
}

@inproceedings{qi2018,
  title = {Universal {{Dependency Parsing}} from {{Scratch}}},
  booktitle = {Proceedings of The},
  author = {Qi, Peng and Dozat, Timothy and Zhang, Yuhao and Manning, Christopher D.},
  year = {2018},
  pages = {160--170},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/k18-2016},
  abstract = {This paper describes Stanford's system at the CoNLL 2018 UD Shared Task. We introduce a complete neural pipeline system that takes raw text as input, and performs all tasks required by the shared task, ranging from tokenization and sentence segmentation, to POS tagging and dependency parsing. Our single system submission achieved very competitive performance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2nd, 1st, and 3rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on lowresource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies.},
  langid = {english},
  file = {/home/cjber/drive/pdf/qi_et_al_2018.pdf}
}

@inproceedings{ratinov2009,
  title = {Design {{Challenges}} and {{Misconceptions}} in {{Named Entity Recognition}}},
  booktitle = {Proceedings of the {{Thirteenth Conference}} on {{Computational Natural Language Learning}} ({{CoNLL-2009}})},
  author = {Ratinov, Lev and Roth, Dan},
  year = {2009},
  month = jun,
  pages = {147--155},
  publisher = {{Association for Computational Linguistics}},
  address = {{Boulder, Colorado}},
  file = {/home/cjber/drive/pdf/ratinov_roth_2009.pdf;/home/cjber/drive/pdf/ratinov_roth_22.pdf}
}

@article{sanh2020,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2020},
  month = feb,
  journal = {arXiv:1910.01108 [cs]},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/sanh_et_al_2020.pdf}
}

@article{see2016,
  title = {Crowdsourcing, {{Citizen Science}} or {{Volunteered Geographic Information}}? {{The Current State}} of {{Crowdsourced Geographic Information}}},
  shorttitle = {Crowdsourcing, {{Citizen Science}} or {{Volunteered Geographic Information}}?},
  author = {See, Linda and Mooney, Peter and Foody, Giles and Bastin, Lucy and Comber, Alexis and Estima, Jacinto and Fritz, Steffen and Kerle, Norman and Jiang, Bin and Laakso, Mari and Liu, Hai-Ying and Mil{\v c}inski, Grega and Nik{\v s}i{\v c}, Matej and Painho, Marco and P{\H o}d{\"o}r, Andrea and {Olteanu-Raimond}, Ana-Maria and Rutzinger, Martin},
  year = {2016},
  month = may,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {5},
  number = {5},
  pages = {55},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/ijgi5050055},
  abstract = {Citizens are increasingly becoming an important source of geographic information, sometimes entering domains that had until recently been the exclusive realm of authoritative agencies. This activity has a very diverse character as it can, amongst other things, be active or passive, involve spatial or aspatial data and the data provided can be variable in terms of key attributes such as format, description and quality. Unsurprisingly, therefore, there are a variety of terms used to describe data arising from citizens. In this article, the expressions used to describe citizen sensing of geographic information are reviewed and their use over time explored, prior to categorizing them and highlighting key issues in the current state of the subject. The latter involved a review of \textasciitilde 100 Internet sites with particular focus on their thematic topic, the nature of the data and issues such as incentives for contributors. This review suggests that most sites involve active rather than passive contribution, with citizens typically motivated by the desire to aid a worthy cause, often receiving little training. As such, this article provides a snapshot of the role of citizens in crowdsourcing geographic information and a guide to the current status of this rapidly emerging and evolving subject.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {citizen science,crowdsourcing,mapping,volunteered geographic information},
  file = {/home/cjber/drive/pdf/see_et_al_2016.pdf;/home/cjber/.zotero/storage/TFTKUQ3P/55.html}
}

@article{speriosu2013,
  title = {Text-{{Driven Toponym Resolution}} Using {{Indirect Supervision}}},
  author = {Speriosu, Michael and Baldridge, Jason},
  year = {2013},
  pages = {10},
  abstract = {Toponym resolvers identify the specific locations referred to by ambiguous placenames in text. Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population. This paper shows that text-driven disambiguation for toponyms is far more effective. We exploit document-level geotags to indirectly generate training instances for text classifiers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/speriosu_baldridge_2013.pdf}
}

@inproceedings{tjongkimsang2003,
  title = {Introduction to the {{CoNLL-2003 Shared Task}}: {{Language-Independent Named Entity Recognition}}},
  shorttitle = {Introduction to the {{CoNLL-2003 Shared Task}}},
  booktitle = {Proceedings of the {{Seventh Conference}} on {{Natural Language Learning}} at {{HLT-NAACL}} 2003},
  author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
  year = {2003},
  pages = {142--147},
  doi = {10.3115/1119176.1119195},
  file = {/home/cjber/drive/pdf/tjong_kim_sang_de_meulder_2003.pdf}
}

@article{twaroch2019,
  title = {Investigating Behavioural and Computational Approaches for Defining Imprecise Regions},
  author = {Twaroch, Florian A. and Brindley, Paul and Clough, Paul D. and Jones, Christopher B. and Pasley, Robert C. and Mansbridge, Sue},
  year = {2019},
  month = apr,
  journal = {Spatial Cognition \& Computation},
  volume = {19},
  number = {2},
  pages = {146--171},
  issn = {1387-5868, 1542-7633},
  doi = {10.1080/13875868.2018.1531871},
  abstract = {Centre from a street survey (with 61 participants) to spatial extents derived from various web-based sources. Such automated approaches have advantages of speed, cost and repeatability. Our results show that footprints derived from web sources are often in concordance with models derived from more labourintensive methods. There were, however, differences between some of the data sources, with those advertising/selling residential property diverging the most from the street survey data. Agreement between sources was measured by aggregating the web sources to identify locations of consensus.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Twaroch et al_2019_Investigating behavioural and computational approaches for defining imprecise.pdf}
}

@article{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/vaswani_et_al_2017.pdf;/home/cjber/.zotero/storage/37NNQGYQ/1706.html}
}

@article{wallgrun2018,
  title = {{{GeoCorpora}}: Building a Corpus to Test and Train Microblog Geoparsers},
  shorttitle = {{{GeoCorpora}}},
  author = {Wallgr{\"u}n, Jan Oliver and Karimzadeh, Morteza and MacEachren, Alan M. and Pezanowski, Scott},
  year = {2018},
  month = jan,
  journal = {International Journal of Geographical Information Science},
  volume = {32},
  number = {1},
  pages = {1--29},
  issn = {1365-8816, 1362-3087},
  doi = {10.1080/13658816.2017.1368523},
  abstract = {In this article, we present the GeoCorpora corpus building framework and software tools as well as a geo-annotated Twitter corpus built with these tools to foster research and development in the areas of microblog/Twitter geoparsing and geographic information retrieval. The developed framework employs crowdsourcing and geovisual analytics to support the construction of large corpora of text in which the mentioned location entities are identified and geolocated to toponyms in existing geographical gazetteers. We describe how the approach has been applied to build a corpus of geo-annotated tweets that will be made freely available to the research community alongside this article to support the evaluation, comparison and training of geoparsers. Additionally, we report lessons learned related to corpus construction for geoparsing as well as insights about the notions of place and natural spatial language that we derive from application of the framework to building this corpus.},
  langid = {english},
  annotation = {24 citations (Crossref) [2022-07-27]},
  file = {/home/cjber/.zotero/storage/6EI3MZ8K/Wallgrün et al_2018_GeoCorpora2.pdf;/home/cjber/.zotero/storage/QBFJXSS2/Wallgrün et al_2018_GeoCorpora3.pdf;/home/cjber/drive/pdf/wallgrun_et_al_2018.pdf}
}

@misc{weischedelralph2013,
  title = {{{OntoNotes Release}} 5.0},
  author = {Weischedel, Ralph and Palmer, Martha and Marcus, Mitchell and Hovy, Eduard and Pradhan, Sameer and Ramshaw, Lance and Xue, Nianwen and Taylor, Ann and Kaufman, Jeff and Franchini, Michelle and {El-Bachouti, Mohammed} and Belvin, Robert and Houston, Ann},
  year = {2013},
  month = oct,
  pages = {2806280 KB},
  publisher = {{Linguistic Data Consortium}},
  doi = {10.35111/XMHB-2B84},
  abstract = {{$<$}h3{$>$}Introduction{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}OntoNotes Release 5.0 is the final release of the OntoNotes project, a collaborative effort between {$<$}a href="http://www.bbn.com/" rel="nofollow"{$>$}BBN Technologies{$<$}/a{$>$}, the {$<$}a href="http://www.colorado.edu/" rel="nofollow"{$>$}University of Colorado{$<$}/a{$>$}, the {$<$}a href="http://www.upenn.edu/" rel="nofollow"{$>$}University of Pennsylvania{$<$}/a{$>$} and the {$<$}a href="http://www.isi.edu/home" rel="nofollow"{$>$}University of Southern Californias Information Sciences Institute{$<$}/a{$>$}. The goal of the project was to annotate a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}OntoNotes Release 5.0 contains the content of earlier releases -- OntoNotes Release 1.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2007T21" rel="nofollow"{$>$}LDC2007T21{$<$}/a{$>$}, OntoNotes Release 2.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2008T04" rel="nofollow"{$>$}LDC2008T04{$<$}/a{$>$}, OntoNotes Release 3.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2009T24" rel="nofollow"{$>$}LDC2009T24{$<$}/a{$>$} and OntoNotes Release 4.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2011T03" rel="nofollow"{$>$}LDC2011T03{$<$}/a{$>$} -- and adds source data from and/or additional annotations for, newswire (News), broadcast news (BN), broadcast conversation (BC), telephone conversation (Tele) and web data (Web) in English and Chinese and newswire data in Arabic. Also contained is English pivot text (Old Testament and New Testament text). This cumulative publication consists of 2.9 million words with counts shown in the table below.{$<$}/p{$><$}br{$>$}  {$<$}table{$><$}br{$>$}  {$<$}tbody{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>\&$}nbsp;{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}Arabic{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}English{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}Chinese{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}News{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}300k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}625k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}250k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}BN{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}200k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}250k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}BC{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}200k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}150k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}Web{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}300k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}150k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}Tele{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}120k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}100k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}Pivot{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}300{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}/tbody{$><$}br{$>$}  {$<$}/table{$><$}br{$>$}  {$<$}p{$>\&$}nbsp;{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}The OntoNotes project built on two time-tested resources, following the {$<$}a href="http://catalog.ldc.upenn.edu/LDC99T42" rel="nofollow"{$>$}Penn Treebank{$<$}/a{$>$} for syntax and the {$<$}a href="http://catalog.ldc.upenn.edu/LDC2004T14" rel="nofollow"{$>$}Penn PropBank{$<$}/a{$>$} for predicate-argument structure. Its semantic representation includes word sense disambiguation for nouns and verbs, with some word senses connected to an ontology, and coreference.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Data{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}Documents describing the annotation guidelines and the routines for deriving various views of the data from the database are included in the documentation directory of this release. The annotation is provided both in separate text files for each annotation layer (Treebank, PropBank, word sense, etc.) and in the form of an integrated relational database (ontonotes-v5.0.sql.gz) with a Python API to provide convenient cross-layer access.{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}It is a known issue that this release contains some non-validating XML files. The included tools, however, use a non-validating XML parser to parse the .xml files and load the appropriate values.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Tools{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}This release includes OntoNotes DB Tool v0.999 beta, the tool used to assemble the database from the original annotation files. It can be found in the directory tools/ontonotes-db-tool-v0.999b. This tool can be used to derive various views of the data from the database, and it provides an API that can implement new queries or views. Licensing information for the OntoNotes DB Tool package is included in its source directory.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Samples{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}Please view these samples:{$<$}/p{$><$}br{$>$}  {$<$}ul{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC2013T19.cmn.jpg" rel="nofollow"{$>$}Chinese{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC2013T19.ara.jpg" rel="nofollow"{$>$}Arabic{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC2013T19.eng.jpg" rel="nofollow"{$>$}English{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}/ul{$><$}br{$>$}  {$<$}h3{$>$}Updates{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}Additional documentation was added on December 11, 2014\&nbsp; and is included in downloads after that date.\&nbsp;{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Acknowledgment{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}This work is supported in part by the Defense Advanced Research Projects Agency, GALE Program Grant No. HR0011-06-1-003. The content of this publication does not necessarily reflect the position or policy of the Government, and no official endorsement should be inferred.{$<$}/p{$><$}/br{$>$}  Portions \textcopyright{} 2006 Abu Dhabi TV, \textcopyright{} 2006 Agence France Presse, \textcopyright{} 2006 Al-Ahram, \textcopyright{} 2006 Al Alam News Channel, \textcopyright{} 2006 Al Arabiya, \textcopyright{} 2006 Al Hayat, \textcopyright{} 2006 Al Iraqiyah, \textcopyright{} 2006 Al Quds-Al Arabi, \textcopyright{} 2006 Anhui TV, \textcopyright{} 2002, 2006 An Nahar, \textcopyright{} 2006 Asharq-al-Awsat, \textcopyright{} 2010 Bible League International, \textcopyright{} 2005 Cable News Network, LP, LLLP, \textcopyright{} 2000-2001 China Broadcasting System, \textcopyright{} 2000-2001, 2005-2006 China Central TV, \textcopyright{} 2006 China Military Online, \textcopyright{} 2000-2001 China National Radio, \textcopyright{} 2006 Chinanews.com, \textcopyright{} 2000-2001 China Television System, \textcopyright{} 1989 Dow Jones \& Company, Inc., \textcopyright{} 2006 Dubai TV, \textcopyright{} 2006 Guangming Daily, \textcopyright{} 2006 Kuwait TV, \textcopyright{} 2005-2006 National Broadcasting Company, Inc., \textcopyright{} 2006 New Tang Dynasty TV, \textcopyright{} 2006 Nile TV, \textcopyright{} 2006 Oman TV, \textcopyright{} 2006 PAC Ltd, \textcopyright{} 2006 Peoples Daily Online, \textcopyright{} 2005-2006 Phoenix TV, \textcopyright{} 2000-2001 Sinorama Magazine, \textcopyright{} 2006 Syria TV, \textcopyright{} 1996-1998, 2006 Xinhua News Agency, \textcopyright{} 1996, 1997, 2005, 2007, 2008, 2009, 2011, 2013 Trustees of the University of Pennsylvania}
}

@inproceedings{weissenbacher2019,
  title = {{{SemEval-2019 Task}} 12: {{Toponym Resolution}} in {{Scientific Papers}}},
  shorttitle = {{{SemEval-2019 Task}} 12},
  booktitle = {Proceedings of the 13th {{International Workshop}} on {{Semantic Evaluation}}},
  author = {Weissenbacher, Davy and Magge, Arjun and O'Connor, Karen and Scotch, Matthew and {Gonzalez-Hernandez}, Graciela},
  year = {2019},
  pages = {907--916},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota, USA}},
  doi = {10/ggwjtv},
  abstract = {We present the SemEval-2019 Task 12 which focuses on toponym resolution in scientific articles. Given an article from PubMed, the task consists of detecting mentions of names of places, or toponyms, and mapping the mentions to their corresponding entries in GeoNames.org, a database of geospatial locations. We proposed three subtasks. In Subtask 1, we asked participants to detect all toponyms in an article. In Subtask 2, given toponym mentions as input, we asked participants to disambiguate them by linking them to entries in GeoNames. In Subtask 3, we asked participants to perform both the detection and the disambiguation steps for all toponyms. A total of 29 teams registered, and 8 teams submitted a system run. We summarize the corpus and the tools created for the challenge. They are freely available at https://competitions.codalab. org/competitions/19948. We also analyze the methods, the results and the errors made by the competing systems with a focus on toponym disambiguation.},
  langid = {english},
  keywords = {TD},
  file = {/home/cjber/drive/pdf/weissenbacher_et_al_2019.pdf;/home/cjber/drive/pdf/weissenbacher_et_al_22.pdf}
}

@article{wolf2020,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State-of-the-art Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = jul,
  journal = {arXiv:1910.03771 [cs]},
  eprint = {1910.03771},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/ huggingface/transformers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/wolf_et_al_2020.pdf}
}
